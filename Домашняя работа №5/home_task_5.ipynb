{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRL86Xl29r9W"
      },
      "source": [
        "# Домашняя работа №5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xNMViGM9wc3"
      },
      "source": [
        "## Задание 1. Написать теггер на данных с русским языком\n",
        "1. Проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
        "2. Написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
        "3. Сравнить все реализованные методы, сделать выводы\n",
        " \n",
        "## Задание 2. Проверить, насколько хорошо работает NER. \n",
        "### Данные брать из http://www.labinform.ru/pub/named_entities/\n",
        "1. Проверить NER из nltk/spacy/deeppavlov.\n",
        "2. Написать свой NER, попробовать разные подходы.\n",
        "  - передаём в сетку токен и его соседей.\n",
        "  - передаём в сетку только токен.\n",
        "  - свой вариант.\n",
        "3. сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjrfjVzk97DW",
        "outputId": "df0d4eb1-f2de-41f3-ad9c-101a1d64ce87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyconll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RxVPSR9a97GA"
      },
      "outputs": [],
      "source": [
        "import pyconll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "p3V0kTd897Ie",
        "outputId": "2d7b5183-d4df-44c3-d2ac-0d8f8cd485a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=7472a97e43ae4dec61858b1a37e26aec20aeb9742485594ea77db387e6907b84\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ru_taiga-ud-dev.conllu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "wget.download('https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-Taiga/master/ru_taiga-ud-train.conllu')\n",
        "wget.download('https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-Taiga/master/ru_taiga-ud-dev.conllu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeTx4V4197LY",
        "outputId": "a8009db4-4ad0-42a5-d7c0-fc83800b9fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "from nltk.tag import DefaultTagger\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tag import BigramTagger, TrigramTagger\n",
        "from nltk.tag import RegexpTagger\n",
        "from nltk.corpus import names\n",
        "import nltk\n",
        "nltk.download('names')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Co5pGBIcC37m"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from scipy.sparse import hstack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tUFRqelQONR5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xxTBkmfe-zgj"
      },
      "outputs": [],
      "source": [
        "full_train = pyconll.load_from_file('ru_taiga-ud-train.conllu')\n",
        "full_test = pyconll.load_from_file('ru_taiga-ud-dev.conllu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SKrvSNB-zjQ",
        "outputId": "758bf937-e3b8-4089-b678-890e9359eb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Снова ADV\n",
            "приобрел VERB\n",
            "дозу NOUN\n",
            ", PUNCT\n",
            "\n",
            "В ADP\n",
            "женщине NOUN\n",
            "важна ADJ\n",
            "верность NOUN\n",
            ", PUNCT\n",
            "а CCONJ\n",
            "не PART\n",
            "красота NOUN\n",
            ". PUNCT\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sent in full_train[:2]:\n",
        "    for token in sent:\n",
        "        print(token.form, token.upos)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npHB-tXf_VJk"
      },
      "source": [
        "### 1.1 проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey6z-oUI-zqw",
        "outputId": "2c6f3a8b-080c-4ff2-9c79-80b621db0441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Наибольшая длина предложения 274\n",
            "Наибольшая длина токена 161\n"
          ]
        }
      ],
      "source": [
        "fdata_train = []\n",
        "for sent in full_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_sent_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])\n",
        "    \n",
        "    \n",
        "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
        "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
        "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
        "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSETqemP-zvg",
        "outputId": "9a4b7ce2-6c57-453b-f428-488018516844"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24167987321711568"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "default_tagger = nltk.DefaultTagger('NOUN')\n",
        "default_tagger.evaluate(fdata_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Ke_BhNA_-Q",
        "outputId": "df2ee29c-0da3-42cc-e85a-e680e41b2fe2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6831418383518225"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "unigram_tagger = UnigramTagger(fdata_train)\n",
        "unigram_tagger.evaluate(fdata_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB8wa_-lBABS",
        "outputId": "0e1b5ff3-b8c7-4ea5-e3ab-0fa8fd4f3b0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6859152139461173"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
        "bigram_tagger.evaluate(fdata_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBASDm92BADc",
        "outputId": "25e0a6e6-f915-41cc-eb6a-6f40b22af253"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6867076069730587"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
        "trigram_tagger.evaluate(fdata_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vKQ2JcuBAF8",
        "outputId": "db674405-4e11-452a-de1e-f2828890acc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.785756735340729"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
        "    for cls in tagger_classes:\n",
        "        backoff = cls(train_sents, backoff=backoff)\n",
        "    return backoff\n",
        "\n",
        "\n",
        "backoff = DefaultTagger('NOUN') \n",
        "tag = backoff_tagger(fdata_train,  \n",
        "                     [\n",
        "                      UnigramTagger, \n",
        "                      BigramTagger, \n",
        "                      TrigramTagger\n",
        "                     ],  \n",
        "                     backoff = backoff) \n",
        "  \n",
        "tag.evaluate(fdata_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwThLuSXCmWp"
      },
      "source": [
        "## 1.2 написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhlaNsDWBAIO",
        "outputId": "58d61b2f-3346-4c69-a51a-90d8c0fa9c03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM',\n",
              "       'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'],\n",
              "      dtype='<U5')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_tok = []\n",
        "train_label = []\n",
        "for sent in fdata_train[:]:\n",
        "    for tok in sent:\n",
        "        train_tok.append(tok[0])\n",
        "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
        "        \n",
        "test_tok = []\n",
        "test_label = []\n",
        "for sent in fdata_test[:]:\n",
        "    for tok in sent:\n",
        "        test_tok.append(tok[0])\n",
        "        test_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
        "        \n",
        "        \n",
        "le = LabelEncoder()\n",
        "train_enc_labels = le.fit_transform(train_label) \n",
        "test_enc_labels = le.transform(test_label)\n",
        "le.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAiXtSRYBAK0",
        "outputId": "323eedb4-059f-40ae-9a41-a19547cdae84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(176631, 93146)\n",
            "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.8331022187004754\n",
            "(176631, 1048576)\n",
            "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.8484548335974643\n",
            "(176631, 93146)\n",
            "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.8544968304278923\n"
          ]
        }
      ],
      "source": [
        "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
        "\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    coder = vectorizer(ngram_range=(1, 5), analyzer='char')\n",
        "    \n",
        "\n",
        "    X_train = coder.fit_transform(train_tok)\n",
        "    X_test = coder.transform(test_tok)\n",
        "    \n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.fit_transform(X_test)    \n",
        "    \n",
        "    \n",
        "    print(X_train.shape)\n",
        "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
        "    lr.fit(X_train, train_enc_labels)\n",
        "\n",
        "    pred = lr.predict(X_test)\n",
        "\n",
        "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU01r_C5BAMt",
        "outputId": "7e4e64bf-2542-4ca9-b6eb-94a987ba9847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(176631, 34451)\n",
            "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.49554278922345485\n",
            "(176631, 1048576)\n",
            "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.5009904912836767\n",
            "(176631, 34451)\n",
            "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.4948494453248811\n"
          ]
        }
      ],
      "source": [
        "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
        "\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    coder = vectorizer(ngram_range=(1, 5), analyzer='word')\n",
        "    \n",
        "\n",
        "    X_train = coder.fit_transform(train_tok)\n",
        "    X_test = coder.transform(test_tok)\n",
        "    \n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.fit_transform(X_test)    \n",
        "    \n",
        "    \n",
        "    print(X_train.shape)\n",
        "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
        "    lr.fit(X_train, train_enc_labels)\n",
        "\n",
        "    pred = lr.predict(X_test)\n",
        "\n",
        "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SpEa4TaCuhf",
        "outputId": "c65c5dce-88f9-4fff-b47f-53069b5558f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(176631, 1141722)\n",
            "TfidfVectorizer_char + HashingVectorizer_word : 0.856675911251981\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler(with_mean=False)\n",
        "coder_1 = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
        "coder_2 = HashingVectorizer(ngram_range=(1, 5), analyzer='word')\n",
        "\n",
        "X_train_1 = coder_1.fit_transform(train_tok)\n",
        "X_test_1 = coder_1.transform(test_tok)\n",
        "\n",
        "X_train_2 = coder_2.fit_transform(train_tok)\n",
        "X_test_2 = coder_2.transform(test_tok)\n",
        "\n",
        "\n",
        "X_train = hstack((X_train_1,X_train_2))\n",
        "X_test = hstack((X_test_1,X_test_2))\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)    \n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
        "lr.fit(X_train, train_enc_labels)\n",
        "\n",
        "pred = lr.predict(X_test)\n",
        "\n",
        "print('TfidfVectorizer_char + HashingVectorizer_word :', accuracy_score(test_enc_labels, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuUHDflfNggP"
      },
      "source": [
        "__Выводы:__\n",
        "\n",
        "Для nltk.tag лучший вариант это: Комбинация из DefaultTagger UnigramTagger BigramTagger TrigramTagger 0.785756735340729\n",
        "\n",
        "Для Vectorizer лучший вариант это: Комбинация из LogisticRegression поверх 'TfidfVectorizer_char + HashingVectorizer_word' 0.856675911251981"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxThe_cF9wR3"
      },
      "source": [
        "## Задание 2. Проверить насколько хорошо работает NER\n",
        "данные брать из http://www.labinform.ru/pub/named_entities/\n",
        "1. взять нер из nltk\n",
        "2. проверить deeppavlov\n",
        "3. написать свой нер попробовать разные подходы:\n",
        " - передаём в сетку токен и его соседей\n",
        " - передаём в сетку только токен\n",
        "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1q-UfUCT1um"
      },
      "source": [
        "## 2.1 взять нер из nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMSLGJad9rG7",
        "outputId": "3d37bedb-cb27-45d9-d4b1-bd799892f266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting corus\n",
            "  Downloading corus-0.9.0-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: corus\n",
            "Successfully installed corus-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install corus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TtjCVUsmOWWX"
      },
      "outputs": [],
      "source": [
        "import corus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "myTuCMoeO6Em"
      },
      "outputs": [],
      "source": [
        "from corus import load_ne5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FipubRb7OWZJ",
        "outputId": "28311151-13fc-4ed6-b5a7-d5589fd679c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'collection5.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "wget.download('http://www.labinform.ru/pub/named_entities/collection5.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0jrg1PUzkqnG"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('collection5.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0Ino_5YQJF1",
        "outputId": "b91c2cb5-5efa-4719-a0d2-43c4b5a52f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcUTAyvKS2Me",
        "outputId": "7b77d0e5-815b-47f6-f4f7-6efa469f4a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection5\t ru_taiga-ud-dev.conllu    sample_data\n",
            "collection5.zip  ru_taiga-ud-train.conllu\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIOoiLrFkqnH"
      },
      "outputs": [],
      "source": [
        "# !unzip collection5.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-IQjcVnRkqnH"
      },
      "outputs": [],
      "source": [
        "from corus import load_ne5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NqI7xBcukqnH"
      },
      "outputs": [],
      "source": [
        "def read_files(file_path):\n",
        "    with open(file_path, encoding='utf8') as f:\n",
        "        text = f.read()\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kSbtz49EQJIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904b0770-1fba-4a0a-faad-fc232eb065bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ne5Markup(\n",
              "    id='last_35',\n",
              "    text='Нового руководителя подмосковной прокуратуры нашли в столице\\r\\n\\r\\n\\r\\nИсполнять обязанности руководителя прокуратуры Московской области с сегодняшнего дня поручено Алексею Захарову, который до сих пор работал заместителем прокурора Москвы. На этом посту он заменит ушедшего в отставку Александра Аникина.\\r\\n\\r\\nПо данным газеты \"Коммерсантъ\", А.Аникин, который занял пост руководителя прокуратуры Московской области после знаменитого \"игорного дела\", за два года заменил 28 городских, районных и специализированных прокуроров, проведя внеплановую переаттестацию сотрудников.\\r\\n\\r\\nОднако проблемы возникли и у самого А.Аникина. По данным издания, у него были разногласия с двумя заместителями генерального прокурора - Виктором Гринем и Владимиром Малиновским. В результате первую попытку уйти в отставку А.Аникин предпринял еще весной, но его просьбу отклонил Юрий Чайка. Теперь же просьба прокурора Подмосковья удовлетворена. Он написал заявление и ушел в 45-дневный отпуск.\\r\\n\\r\\nЧто касается А.Захарова, то он является уроженцем Архангельска, образование получил в Саратове, а в московской прокуратуре работал с 2010г. С Московской областью он знаком не понаслышке. В 1996-2000гг. А.Захаров трудился в Раменской городской прокуратуре, а также в Жуковском.\\r\\n\\r\\nВ 2001-2009гг. А.Захаров работал в Басманной межрайонной прокуратуре Москвы.\\r\\n',\n",
              "    spans=[Ne5Span(\n",
              "         index='T1',\n",
              "         type='LOC',\n",
              "         start=113,\n",
              "         stop=131,\n",
              "         text='Московской области'\n",
              "     ), Ne5Span(\n",
              "         index='T2',\n",
              "         type='PER',\n",
              "         start=160,\n",
              "         stop=176,\n",
              "         text='Алексею Захарову'\n",
              "     ), Ne5Span(\n",
              "         index='T3',\n",
              "         type='LOC',\n",
              "         start=228,\n",
              "         stop=234,\n",
              "         text='Москвы'\n",
              "     ), Ne5Span(\n",
              "         index='T4',\n",
              "         type='PER',\n",
              "         start=281,\n",
              "         stop=299,\n",
              "         text='Александра Аникина'\n",
              "     ), Ne5Span(\n",
              "         index='T5',\n",
              "         type='MEDIA',\n",
              "         start=322,\n",
              "         stop=333,\n",
              "         text='Коммерсантъ'\n",
              "     ), Ne5Span(\n",
              "         index='T6',\n",
              "         type='PER',\n",
              "         start=336,\n",
              "         stop=344,\n",
              "         text='А.Аникин'\n",
              "     ), Ne5Span(\n",
              "         index='T7',\n",
              "         type='LOC',\n",
              "         start=390,\n",
              "         stop=408,\n",
              "         text='Московской области'\n",
              "     ), Ne5Span(\n",
              "         index='T8',\n",
              "         type='PER',\n",
              "         start=607,\n",
              "         stop=616,\n",
              "         text='А.Аникина'\n",
              "     ), Ne5Span(\n",
              "         index='T9',\n",
              "         type='PER',\n",
              "         start=708,\n",
              "         stop=723,\n",
              "         text='Виктором Гринем'\n",
              "     ), Ne5Span(\n",
              "         index='T10',\n",
              "         type='PER',\n",
              "         start=726,\n",
              "         stop=748,\n",
              "         text='Владимиром Малиновским'\n",
              "     ), Ne5Span(\n",
              "         index='T11',\n",
              "         type='PER',\n",
              "         start=794,\n",
              "         stop=802,\n",
              "         text='А.Аникин'\n",
              "     ), Ne5Span(\n",
              "         index='T12',\n",
              "         type='PER',\n",
              "         start=850,\n",
              "         stop=860,\n",
              "         text='Юрий Чайка'\n",
              "     ), Ne5Span(\n",
              "         index='T13',\n",
              "         type='LOC',\n",
              "         start=890,\n",
              "         stop=901,\n",
              "         text='Подмосковья'\n",
              "     ), Ne5Span(\n",
              "         index='T14',\n",
              "         type='PER',\n",
              "         start=982,\n",
              "         stop=992,\n",
              "         text='А.Захарова'\n",
              "     ), Ne5Span(\n",
              "         index='T15',\n",
              "         type='LOC',\n",
              "         start=1019,\n",
              "         stop=1031,\n",
              "         text='Архангельска'\n",
              "     ), Ne5Span(\n",
              "         index='T16',\n",
              "         type='LOC',\n",
              "         start=1055,\n",
              "         stop=1063,\n",
              "         text='Саратове'\n",
              "     ), Ne5Span(\n",
              "         index='T17',\n",
              "         type='LOC',\n",
              "         start=1111,\n",
              "         stop=1130,\n",
              "         text='Московской областью'\n",
              "     ), Ne5Span(\n",
              "         index='T18',\n",
              "         type='PER',\n",
              "         start=1171,\n",
              "         stop=1180,\n",
              "         text='А.Захаров'\n",
              "     ), Ne5Span(\n",
              "         index='T19',\n",
              "         type='ORG',\n",
              "         start=1192,\n",
              "         stop=1223,\n",
              "         text='Раменской городской прокуратуре'\n",
              "     ), Ne5Span(\n",
              "         index='T20',\n",
              "         type='LOC',\n",
              "         start=1235,\n",
              "         stop=1244,\n",
              "         text='Жуковском'\n",
              "     ), Ne5Span(\n",
              "         index='T21',\n",
              "         type='PER',\n",
              "         start=1264,\n",
              "         stop=1273,\n",
              "         text='А.Захаров'\n",
              "     ), Ne5Span(\n",
              "         index='T22',\n",
              "         type='LOC',\n",
              "         start=1318,\n",
              "         stop=1324,\n",
              "         text='Москвы'\n",
              "     )]\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "records = load_ne5('Collection5/')\n",
        "document = next(records)\n",
        "document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hd8osbskqnL",
        "outputId": "17df43db-e61f-4982-d5bb-2b6324781610"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object load_ne5 at 0x7f10f458a650>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbMfR0rASlut",
        "outputId": "ea514941-5525-4173-a8dc-beec25f97a11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('Tamarod', 'ORGANIZATION'),\n",
              " ('Абдель Азиз', 'PERSON'),\n",
              " ('Дауд', 'PERSON'),\n",
              " ('Египетская', 'PERSON'),\n",
              " ('Египта', 'PERSON'),\n",
              " ('Египте', 'PERSON'),\n",
              " ('Мохамед Абдель Азиз', 'PERSON'),\n",
              " ('Совета Шуры', 'PERSON'),\n",
              " ('Фронт', 'PERSON')}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJt3uMd7T47m"
      },
      "source": [
        "## 2.2. проверить deeppavlov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "log7mU1VSlxg"
      },
      "outputs": [],
      "source": [
        "# !pip install deeppavlov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyLp6TkhkqnO"
      },
      "outputs": [],
      "source": [
        "# не пошла установка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYu_1z9OUZF4"
      },
      "source": [
        "## 2.3 написать свой нер попробовать разные подходы:\n",
        "- передаём в сетку токен и его соседей\n",
        "- передаём в сетку только токен"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR2sPTFOVX2h",
        "outputId": "08e9a357-b02b-4b1d-b130-7cd08debae1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install razdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MMhyzUlVSl2Y"
      },
      "outputs": [],
      "source": [
        "from razdel import tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mAJjv18i6QWk"
      },
      "outputs": [],
      "source": [
        "records = corus.load_ne5('Collection5/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "P_lBecpdkqnQ"
      },
      "outputs": [],
      "source": [
        "words_docs = []\n",
        "for ix, rec in enumerate(records):\n",
        "    words = []\n",
        "    for token in tokenize(rec.text):\n",
        "       \n",
        "        result = 'None'        \n",
        "        \n",
        "        for item in rec.spans:            \n",
        "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'PER'):\n",
        "                result = 'PER'\n",
        "                break\n",
        "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'ORG'):\n",
        "                result = 'ORG'\n",
        "                break\n",
        "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'MEDIA'):\n",
        "                result = 'MEDIA'\n",
        "                break\n",
        "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'LOC'):\n",
        "                result = 'LOC'\n",
        "                break\n",
        "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'GEOPOLIT'):\n",
        "                result = 'GEOPOLIT'\n",
        "                break\n",
        "                \n",
        "    \n",
        "        words.append([token.text, result])\n",
        "    words_docs.extend(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1k8RAMja6QZo"
      },
      "outputs": [],
      "source": [
        "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyYYKXfp6QcR",
        "outputId": "59937d4a-96b1-405c-d8b4-af54daaa4814"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "None        219214\n",
              "PER          21200\n",
              "ORG          13651\n",
              "LOC           4568\n",
              "GEOPOLIT      4356\n",
              "MEDIA         2482\n",
              "Name: tag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "df_words['tag'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "2IIOlMJW6Qey",
        "outputId": "a8ed631e-d5ae-4325-f124-7feec51c811d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                word       tag\n",
              "265461             и       ORG\n",
              "265462  стабилизации       ORG\n",
              "265463       Госдепа       ORG\n",
              "265464           США  GEOPOLIT\n",
              "265465             и      None\n",
              "265466       работал      None\n",
              "265467        послом      None\n",
              "265468            на      None\n",
              "265469       Украине  GEOPOLIT\n",
              "265470             .      None"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-775f714f-a34a-4f86-904e-041c016742d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>265461</th>\n",
              "      <td>и</td>\n",
              "      <td>ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265462</th>\n",
              "      <td>стабилизации</td>\n",
              "      <td>ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265463</th>\n",
              "      <td>Госдепа</td>\n",
              "      <td>ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265464</th>\n",
              "      <td>США</td>\n",
              "      <td>GEOPOLIT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265465</th>\n",
              "      <td>и</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265466</th>\n",
              "      <td>работал</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265467</th>\n",
              "      <td>послом</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265468</th>\n",
              "      <td>на</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265469</th>\n",
              "      <td>Украине</td>\n",
              "      <td>GEOPOLIT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265470</th>\n",
              "      <td>.</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-775f714f-a34a-4f86-904e-041c016742d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-775f714f-a34a-4f86-904e-041c016742d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-775f714f-a34a-4f86-904e-041c016742d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "df_words.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NBnCTkAL6QhS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xdVw5yXg6QjJ"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
        "\n",
        "# labelEncode целевую переменную\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ZJjCF01R6QlO"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
        "\n",
        "train_data = train_data.batch(16)\n",
        "valid_data = valid_data.batch(16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "GcJOq4vxWc6r"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "2xt16YZCWc9H"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "        return input_data\n",
        "\n",
        "def data_prep(train_data, seq_len=1, vocab_size = 30000):    \n",
        "    \n",
        "    vocab_size = 30000\n",
        "    #seq_len = 1\n",
        "\n",
        "    vectorize_layer = TextVectorization(\n",
        "        standardize=custom_standardization,\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode='int',\n",
        "        output_sequence_length=seq_len)\n",
        "\n",
        "\n",
        "    # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "    text_data = train_data.map(lambda x, y: x)\n",
        "    vectorize_layer.adapt(text_data)\n",
        "    return vectorize_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "htTHBq75Vpyy"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 64\n",
        "\n",
        "class modelNER(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(modelNER, self).__init__()\n",
        "        self.emb = Embedding(vocab_size, embedding_dim)\n",
        "        self.gPool = GlobalMaxPooling1D()\n",
        "        self.fc1 = Dense(300, activation='relu')\n",
        "        self.fc2 = Dense(50, activation='relu')\n",
        "        self.fc3 = Dense(len(df_words['tag'].value_counts()), activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = vectorize_layer(x)\n",
        "        x = self.emb(x)\n",
        "        pool_x = self.gPool(x)\n",
        "        \n",
        "        fc_x = self.fc1(pool_x)\n",
        "        fc_x = self.fc2(fc_x)\n",
        "        \n",
        "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
        "        return self.fc3(concat_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3nZoESDVp1X",
        "outputId": "540bc318-21ad-44b7-a459-2550d335fa72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "12444/12444 [==============================] - 62s 5ms/step - loss: 0.2822 - accuracy: 0.9178 - val_loss: 0.2112 - val_accuracy: 0.9384\n",
            "Epoch 2/5\n",
            "12444/12444 [==============================] - 59s 5ms/step - loss: 0.1205 - accuracy: 0.9638 - val_loss: 0.2194 - val_accuracy: 0.9395\n",
            "Epoch 3/5\n",
            "12444/12444 [==============================] - 57s 5ms/step - loss: 0.1064 - accuracy: 0.9660 - val_loss: 0.2278 - val_accuracy: 0.9400\n",
            "Epoch 4/5\n",
            "12444/12444 [==============================] - 58s 5ms/step - loss: 0.1018 - accuracy: 0.9667 - val_loss: 0.2728 - val_accuracy: 0.8813\n",
            "Epoch 5/5\n",
            "12444/12444 [==============================] - 58s 5ms/step - loss: 0.0993 - accuracy: 0.9673 - val_loss: 0.3008 - val_accuracy: 0.8766\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f10800809d0>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "vocab_size = 30000\n",
        "vectorize_layer = data_prep(train_data, seq_len = 1, vocab_size = vocab_size)\n",
        "\n",
        "\n",
        "mmodel = modelNER()\n",
        "mmodel.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "mmodel.fit(train_data, validation_data=valid_data, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVG4rNAZVp4G",
        "outputId": "fbdefcf7-cc97-4460-e3c7-b6453b363524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "12444/12444 [==============================] - 57s 5ms/step - loss: 0.2912 - accuracy: 0.9145 - val_loss: 0.2113 - val_accuracy: 0.9370\n",
            "Epoch 2/5\n",
            "12444/12444 [==============================] - 57s 5ms/step - loss: 0.1225 - accuracy: 0.9631 - val_loss: 0.2128 - val_accuracy: 0.9396\n",
            "Epoch 3/5\n",
            "12444/12444 [==============================] - 59s 5ms/step - loss: 0.1078 - accuracy: 0.9658 - val_loss: 0.2137 - val_accuracy: 0.9406\n",
            "Epoch 4/5\n",
            "12444/12444 [==============================] - 57s 5ms/step - loss: 0.1027 - accuracy: 0.9667 - val_loss: 0.2218 - val_accuracy: 0.9398\n",
            "Epoch 5/5\n",
            "12444/12444 [==============================] - 59s 5ms/step - loss: 0.0997 - accuracy: 0.9674 - val_loss: 0.2368 - val_accuracy: 0.9391\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f102c3b0950>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "vocab_size = 30000\n",
        "vectorize_layer = data_prep(train_data, seq_len = 3, vocab_size = vocab_size)\n",
        "\n",
        "\n",
        "mmodel = modelNER()\n",
        "mmodel.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "mmodel.fit(train_data, validation_data=valid_data, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG6xCuD_a8TP"
      },
      "source": [
        "#### Вывод.\n",
        "\n",
        "Длина последовательности практически не влияет на результат"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('DS_in_medicine_2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "7d46b9b22cbe53d9ec02beae01d2f3ecc8a685b7260fafa7ecede5f88e02afe9"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}